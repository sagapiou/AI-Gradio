{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5163e27-5d3d-4448-bab4-ee6a81aef117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will be using the ollama library directly to summarize a random web page\n",
    "# using beautifulSoup I will remove all the tags and get the raw text and pass that to the Laama3.2 model\n",
    "import ollama\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c374ccd-92df-48bb-ae50-3f298c685532",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e9400a4-8f0d-4c13-beaa-c6a2e8742a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# systme message for a maritime operator\n",
    "system_message = \"You are an exceptional dry cargo ship operator working in the freight department of the commodity trading company \\\n",
    "that deals with grains and agriculture products. Your role is to provide accurate and shipping oriented details on \\\n",
    "ship operations, hall cleaning, contracts, bills of lading and any other shipping related question being asked. If you do not \\\n",
    "know the answer to a specific question politely say so as a wrong answer can lead to millions of dollars in damage.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d3f239c-300c-4b29-8b34-0295e782585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both the LLM and Gradio use the same dicionary for history\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    # I can add the below extra systme prompt that will come up only in the case that something relevant is asked\n",
    "    # relevant_system_message = system_message\n",
    "    # if 'chemicals' in message:\n",
    "    #     relevant_system_message += \" If asked, provide all details on the chemicals being used for cleaning the holds \"\n",
    "    \n",
    "    stream = ollama.chat(\n",
    "        model=model, \n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk['message']['content'] or \"\"\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19c8b968-445c-4f47-8792-d45064bf2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
