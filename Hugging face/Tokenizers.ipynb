{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb49744-544f-4b9e-9eb6-03d89e7572fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa038371-fc20-4cf4-ac06-d21b0dcc519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before running the tokenizer, there is a chance that a login is needed. If so login with the token received from hugging face\n",
    "# login(\"token\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B', trust_remote_code=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53e1b0cb-390c-41d4-911a-f419490c0006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000,\n",
       " 791,\n",
       " 51635,\n",
       " 835,\n",
       " 1887,\n",
       " 374,\n",
       " 72618,\n",
       " 311,\n",
       " 11204,\n",
       " 958,\n",
       " 70436,\n",
       " 3300,\n",
       " 1511,\n",
       " 555,\n",
       " 41283,\n",
       " 5469,\n",
       " 323,\n",
       " 16682,\n",
       " 437,\n",
       " 13,\n",
       " 445,\n",
       " 12270,\n",
       " 6691,\n",
       " 71574,\n",
       " 388,\n",
       " 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here I initially encode the text below with the tokenizer that llama-3.2-1B uses\n",
    "# Autotokenizer above ensures that the tokenizer is the correct one for this model\n",
    "text = \"The FIAT system is collapsing to magic interenet money used by nerds and weirdos. LFG motherfuckers!\"\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f80e7996-a633-4dd4-a58e-92336de4d9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>The FIAT system is collapsing to magic interenet money used by nerds and weirdos. LFG motherfuckers!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoding the tokenized tokens above we can see that we get the initial text along with some special tokens\n",
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0418e392-44e9-42ce-983d-76b3d0c1194a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>',\n",
       " 'The',\n",
       " ' FI',\n",
       " 'AT',\n",
       " ' system',\n",
       " ' is',\n",
       " ' collapsing',\n",
       " ' to',\n",
       " ' magic',\n",
       " ' inter',\n",
       " 'enet',\n",
       " ' money',\n",
       " ' used',\n",
       " ' by',\n",
       " ' ner',\n",
       " 'ds',\n",
       " ' and',\n",
       " ' weird',\n",
       " 'os',\n",
       " '.',\n",
       " ' L',\n",
       " 'FG',\n",
       " ' mother',\n",
       " 'fuck',\n",
       " 'ers',\n",
       " '!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we can see the decoded text in token format\n",
    "tokenizer.batch_decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dfa40eb-6fa3-4bd3-a406-908a956dbc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab shoes the whole token vocabulary\n",
    "# get_added_vocab shows the special tokens like for example the begin_of_text 128000 above\n",
    "# tokenizer.vocab\n",
    "#tokenizer.get_added_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d3ef767-2bae-4751-8578-8ddfe73ed223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b64be04966d4b8ca0de66a38e0d5a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darks\\anaconda3\\envs\\llms\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Darks\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-1B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc5d4f8acdb4602bee2df61de1c0534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348f3431899d413a99a943d9b5f79ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now I will try the tokenizer with an actual prediction model\n",
    "# will use the same llama model but this time the instruct model. instruct are specific to chat bots so they expect a system-user-assistant type of message\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B-Instruct', trust_remote_code=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a944a9b5-7c50-472b-9e54-905b03e3148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Feb 2025\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell a funny-technical joke for a crowd of IT people<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply_chat_template is used here in order to take messages and tokenize it in the form that the model expects for a chatbot\n",
    "# in the propmpt it finishes exactly at the point that is waiting for the assistant to respond. Even the specia tokens for the assistant are there\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell a funny-technical joke for a crowd of IT people\"}\n",
    "  ]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d9599c7-611b-4b3f-b318-17c95a62bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the above are specifice for the specific llama model. Different models have completely different logic in tokens or special tokens etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2115e305-8f39-4232-b9c9-3da187607cad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
